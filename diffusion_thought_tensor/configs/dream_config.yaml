# DREAM Model Configuration - Enhanced Bidirectional Diffusion
# Optimized for DREAM architecture with enhanced thought system

model:
  name: "StackedDiffusionModel3D"
  embed_dim: 720              # Optimized for ~1B total parameters
  num_layers: 16              # Reduced layers to control param count
  num_heads: 16               # 12 heads, 60 dim each (720/12=60)
  vocab_size: 50257           # GPT-2 vocabulary
  max_seq_length: 2048        # Increased context length
  max_new_tokens: 1024         # Longer generation capability
  dropout: 0.1                # Standard dropout
  
# DREAM-specific configurations (integrated into StackedDiffusionModel3D)
dream:
  bidirectional_attention: true     # Built into 3D model
  adaptive_noise_scheduling: false  # Not supported by 3D model
  enhanced_confidence: false        # Not supported by 3D model
  block_wise_unmasking: true        # Supported via masking strategy
  generation_order_embedding: false # Not needed for 3D model
  pad_token_id: 50256               # GPT2's eos_token_id used as pad_token
  
# Enhanced thought system for StackedDiffusionModel3D
thought_tensor:
  input_dims: [16, 16, 64]         # Reduced 3D format: [H, W, channels] for fair comparison
  stack_size: 8                    # Reduced thought stack depth
  hidden_dim: 720                  # Match embed_dim for consistency
  dropout: 0.15                    # Thought-specific dropout
  noise_injection_std: 0.03       # Reduced noise for stability
  diversity_loss_weight: 0.1      # Encourage diversity
  self_supervised_learning: true   # Enable self-learning
  temporal_dynamics: true          # Velocity/acceleration tracking
  importance_based_retention: true # Smart thought retention
  
# Masking strategy for DREAM
masking:
  mask_ratio: 0.7                  # Initial masking ratio
  confidence_threshold: 0.75       # Unmasking threshold
  progressive_unmasking: true      # Enable progressive strategy
  block_size: 4                    # Block-wise unmasking size
  adaptive_masking: true           # Context-aware masking
  final_mask_ratio: 0.35           # Final masking ratio (mask_ratio * 0.5)
  final_confidence_threshold: 0.9  # Final confidence threshold (min(0.9, threshold + 0.15))
  
# Diffusion configuration optimized for DREAM
diffusion:
  num_steps: 500                   # Reduced steps for efficiency
  noise_schedule: "cosine"         # Optimal schedule
  schedule_type: "cosine"          
  beta_start: 0.0001
  beta_end: 0.015                  # Reduced for stability
  prediction_type: "epsilon"
  adaptive_timesteps: true         # Per-token timesteps
  
# Training optimized for DREAM
training:
  batch_size: 1                    # Small batch for 1B model memory constraints
  gradient_accumulation_steps: 16  # Larger accumulation for effective batch = 16
  learning_rate: 1e-5              # Lower LR for large model stability
  warmup_steps: 2000               # Longer warmup for 1B model
  num_epochs: 5                    # Fewer epochs for large model
  eval_every: 1                    # Regular evaluation
  save_every: 1                    # Save each epoch
  max_grad_norm: 1.0               # Higher gradient clipping for large model
  stack_reset_frequency: 0         # Reset once per epoch
  label_smoothing: 0.1             # More smoothing for large model
  early_stopping_patience: 3      # Keep patience setting
  
# Optimizer for DREAM
optimizer:
  type: "AdamW"
  weight_decay: 0.005              # Reduced weight decay
  betas: [0.9, 0.95]               # Match training script
  eps: 1e-8                        # Match training script
  
# Hardware optimization for DREAM
hardware:
  device: "cuda"
  mixed_precision: true            # Essential for 1B model
  gradient_checkpointing: true     # Critical memory efficiency for 1B
  num_workers: 0                   # Single worker for stability
  dataloader_pin_memory: false     # Disable to save memory
  compile_model: false             # Keep disabled for 1B model
  target_memory_gb: 40             # Higher memory target for 1B model
  
# Memory management for DREAM
memory:
  offload_to_cpu: false
  cpu_offload_params: false
  activation_checkpointing: true
  thought_stack_checkpointing: true # New: checkpoint thought computations
  
# Data configuration
data_config:
  cache_dir: "data/cache"
  data_source: "tinystories"       # Proven working dataset
  train_samples: 100000            # Larger dataset for 1B model
  eval_samples: 10000              # Larger evaluation set
  
# DREAM-specific advanced features
advanced:
  use_flash_attention: false       # Disable for compatibility
  compile_model: false             # Keep disabled
  deepspeed_config: null
  dream_features: true             # Enable all DREAM features
  enhanced_thought_system: true    # Enable enhanced thoughts
  temporal_attention: true         # Multi-scale temporal processing
  
# Enhanced logging for DREAM
logging:
  log_every: 25                    # Frequent logging for DREAM
  use_wandb: false                 # Disabled by default
  use_tensorboard: true            # TensorBoard enabled
  use_dream_metrics: true          # DREAM-specific metrics
  use_advanced_thought_metrics: true
  wandb_project: "dream-diffusion-thought-tensor"
  wandb_tags: ["dream", "bidirectional", "enhanced-thoughts"]
  save_samples: true
  save_thought_visualizations: true
  save_attention_maps: true        # New: save attention patterns
  log_train_every: 50              # Training metrics logging frequency
  thought_histogram_every: 200     # Thought activation histogram frequency
  
# Checkpointing for DREAM
checkpointing:
  checkpoint_dir: "outputs/dream_checkpoints"
  log_dir: "outputs/dream_logs"
  keep_best: true
  keep_latest: 5                   # Keep more checkpoints
  save_optimizer_state: true      # Full state saving
  
# Performance benchmarking
benchmarking:
  enable_benchmarks: true          # Enable performance tracking
  benchmark_every: 5               # Benchmark every 5 epochs
  save_benchmark_results: true     # Save performance data
  
# Model specifications
estimated_params: "~1B"           # StackedDiffusionModel3D scaled to 1B parameters
estimated_vram: "~24GB"           # Memory estimate for 1B model with 3D convolutions
model_type: "stacked_3d_dream_1b" # Model identifier for 1B scale

# Loss configuration for StackedDiffusionModel3D
loss:
  main_loss_weight: 1.0           # Standard diffusion loss
  thought_loss_weight: 0.0        # No direct thought losses (self-supervised)
  confidence_loss_weight: 0.0     # Not supported by 3D model
  masking_loss_weight: 0.02       # Masking strategy loss
  temporal_loss_weight: 0.03      # Temporal consistency loss