# Phase 2 Configuration - Scale to ~500M parameters
# Optimized for RTX 3090 (24GB VRAM)

model:
  name: "Phase2DiffusionThoughtModel"
  embed_dim: 896         # Reduced to target ~500M
  num_layers: 18         # Reduced to target ~500M
  num_heads: 16          # Increased from 12
  vocab_size: 50257      # GPT-2 vocabulary
  max_seq_length: 512    # Increased from 256
  dropout: 0.1
  
thought_tensor:
  input_dims: [32, 32, 16]   # Keep same for compatibility
  output_dims: [32, 32, 16]  # No compression for full model
  stack_size: 16             # Increased from 8
  hidden_dim: 896            # Match embed_dim
  
diffusion:
  num_steps: 1000      # Increased for better quality
  noise_schedule: "cosine"  # Better schedule for large models
  beta_start: 0.0001
  beta_end: 0.02
  
training:
  batch_size: 2              # Reduced for memory
  gradient_accumulation_steps: 16  # Increased to maintain effective batch size of 32
  learning_rate: 5e-5        # Reduced for stability
  warmup_steps: 10000        # Increased warmup
  total_steps: 500000        # Phase 2 target
  eval_every: 5000
  save_every: 25000
  max_grad_norm: 1.0
  
optimizer:
  type: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.95]    # Better for large models
  eps: 1e-6
  
hardware:
  device: "cuda"
  mixed_precision: true
  gradient_checkpointing: true
  num_workers: 2        # Reduced to save memory
  dataloader_pin_memory: false  # Save memory
  
# Memory optimization
memory:
  offload_to_cpu: false
  cpu_offload_params: false
  activation_checkpointing: true
  
# Advanced features for Phase 2
advanced:
  use_flash_attention: false    # Set to true if available
  compile_model: false          # Set to true for PyTorch 2.0+
  deepspeed_config: null        # Optional DeepSpeed config