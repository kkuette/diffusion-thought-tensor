# Baseline Model Configuration - Identical to DREAM config but WITHOUT thought system
# This serves as the control group for testing thought contribution

model:
  name: "BaselineDreamModel"
  embed_dim: 720              # IDENTICAL to thought model
  num_layers: 16              # IDENTICAL to thought model
  num_heads: 16               # IDENTICAL to thought model
  vocab_size: 50257           # IDENTICAL to thought model
  max_seq_length: 2048        # IDENTICAL to thought model
  max_new_tokens: 1024        # IDENTICAL to thought model
  dropout: 0.1                # IDENTICAL to thought model
  
# DREAM-specific configurations (same as thought model)
dream:
  bidirectional_attention: true     # IDENTICAL
  adaptive_noise_scheduling: false  # IDENTICAL
  enhanced_confidence: false        # IDENTICAL
  block_wise_unmasking: true        # IDENTICAL
  generation_order_embedding: false # IDENTICAL
  pad_token_id: 50256               # IDENTICAL

# NO thought_tensor section - this is the key difference!
# The baseline model has NO thought system at all

# Masking strategy for DREAM (IDENTICAL to thought model)
masking:
  mask_ratio: 0.7                  # IDENTICAL
  confidence_threshold: 0.75       # IDENTICAL
  progressive_unmasking: true      # IDENTICAL
  block_size: 4                    # IDENTICAL
  adaptive_masking: true           # IDENTICAL
  final_mask_ratio: 0.35           # IDENTICAL
  final_confidence_threshold: 0.9  # IDENTICAL
  
# Diffusion configuration (IDENTICAL to thought model)
diffusion:
  num_steps: 500                   # IDENTICAL
  noise_schedule: "cosine"         # IDENTICAL
  schedule_type: "cosine"          # IDENTICAL
  beta_start: 0.0001               # IDENTICAL
  beta_end: 0.015                  # IDENTICAL
  prediction_type: "epsilon"       # IDENTICAL
  adaptive_timesteps: true         # IDENTICAL
  
# Training (IDENTICAL to thought model for fair comparison)
training:
  batch_size: 1                    # IDENTICAL - same memory constraints
  gradient_accumulation_steps: 16  # IDENTICAL - same effective batch size
  learning_rate: 1e-5              # IDENTICAL - same learning dynamics
  warmup_steps: 2000               # IDENTICAL - same warmup schedule
  num_epochs: 5                    # IDENTICAL - same training duration
  eval_every: 1                    # IDENTICAL - same evaluation frequency
  save_every: 1                    # IDENTICAL - same checkpointing
  max_grad_norm: 1.0               # IDENTICAL - same gradient clipping
  stack_reset_frequency: 0         # N/A for baseline (no stack)
  label_smoothing: 0.1             # IDENTICAL - same regularization
  early_stopping_patience: 3       # IDENTICAL - same stopping criteria
  
# Optimizer (IDENTICAL to thought model)
optimizer:
  type: "AdamW"                    # IDENTICAL
  weight_decay: 0.005              # IDENTICAL
  betas: [0.9, 0.95]               # IDENTICAL
  eps: 1e-8                        # IDENTICAL
  
# Hardware optimization (IDENTICAL to thought model)
hardware:
  device: "cuda"                   # IDENTICAL
  mixed_precision: true            # IDENTICAL
  gradient_checkpointing: true     # IDENTICAL
  num_workers: 0                   # IDENTICAL
  dataloader_pin_memory: false     # IDENTICAL
  compile_model: false             # IDENTICAL
  target_memory_gb: 40             # IDENTICAL
  
# Memory management (IDENTICAL to thought model)
memory:
  offload_to_cpu: false            # IDENTICAL
  cpu_offload_params: false        # IDENTICAL
  activation_checkpointing: true   # IDENTICAL
  thought_stack_checkpointing: false # N/A for baseline (no thoughts)
  
# Data configuration (IDENTICAL for fair comparison)
data_config:
  cache_dir: "data/cache"          # IDENTICAL
  data_source: "tinystories"       # IDENTICAL - same dataset
  train_samples: 100000            # IDENTICAL - same data size
  eval_samples: 10000              # IDENTICAL - same evaluation set
  
# Advanced features (IDENTICAL except thought-specific ones)
advanced:
  use_flash_attention: false       # IDENTICAL
  compile_model: false             # IDENTICAL
  deepspeed_config: null           # IDENTICAL
  dream_features: true             # IDENTICAL - keep DREAM features
  enhanced_thought_system: false   # DISABLED - this is the key difference!
  temporal_attention: false        # DISABLED - thought-related feature
  
# Logging configuration (separate directories for comparison)
logging:
  log_every: 25                    # IDENTICAL frequency
  use_wandb: false                 # IDENTICAL
  use_tensorboard: true            # IDENTICAL
  use_dream_metrics: true          # IDENTICAL
  use_advanced_thought_metrics: false # DISABLED - no thoughts to analyze
  wandb_project: "baseline-diffusion-thought-tensor"  # Different project name
  wandb_tags: ["baseline", "no-thoughts", "control-group"]  # Different tags
  save_samples: true               # IDENTICAL
  save_thought_visualizations: false # DISABLED - no thoughts to visualize
  save_attention_maps: true        # IDENTICAL
  log_train_every: 50              # IDENTICAL
  thought_histogram_every: 0       # DISABLED - no thoughts
  
# Checkpointing (separate directories for comparison)
checkpointing:
  checkpoint_dir: "outputs/baseline_checkpoints"  # Different directory
  log_dir: "outputs/baseline_logs"                # Different directory
  keep_best: true                  # IDENTICAL
  keep_latest: 5                   # IDENTICAL
  save_optimizer_state: true       # IDENTICAL
  
# Performance benchmarking
benchmarking:
  enable_benchmarks: true          # IDENTICAL
  benchmark_every: 5               # IDENTICAL
  save_benchmark_results: true     # IDENTICAL
  
# Model specifications (expected to be smaller without thoughts)
estimated_params: "~800M"          # SMALLER - no thought system overhead
estimated_vram: "~20GB"            # SMALLER - no 3D convolutions
model_type: "baseline_dream_800m"  # Different identifier

# Loss configuration (simpler without thought losses)
loss:
  main_loss_weight: 1.0           # IDENTICAL
  thought_loss_weight: 0.0        # N/A - no thoughts
  confidence_loss_weight: 0.0     # IDENTICAL - not supported anyway
  masking_loss_weight: 0.02       # IDENTICAL
  temporal_loss_weight: 0.0       # DISABLED - thought-related

# Experimental control settings
experiment:
  control_group: true             # Mark as control group
  comparison_with: "thought_model" # What we're comparing against
  hypothesis: "thoughts_dont_contribute"  # Null hypothesis
  random_seed: 42                 # Fixed seed for reproducibility