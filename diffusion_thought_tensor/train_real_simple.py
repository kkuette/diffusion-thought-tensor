"""
Simple Real Data Training - Fast and Direct
Uses the minimal model with real but fast data generation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
import os
import json
from datetime import datetime
import time

# Import the working minimal model
from model.base_model import DiffusionThoughtModel, count_parameters
from utils.noise_schedules import create_noise_scheduler


class FastRealDataset(Dataset):
    """Fast real-like dataset for quick training tests"""
    
    def __init__(self, num_samples=1000, seq_length=64, vocab_size=1000):
        self.num_samples = num_samples
        self.seq_length = seq_length
        self.vocab_size = vocab_size
        
        print(f"Generating {num_samples} fast samples...")
        
        # Generate simple but realistic patterns quickly
        self.texts = []
        self.thoughts = []
        
        # Simple sentence patterns
        patterns = [
            [1, 2, 3, 4, 5],  # "The cat sat on mat"
            [6, 7, 8, 9],     # "Dog runs very fast"  
            [10, 11, 12],     # "Sun is bright"
            [13, 14, 15, 16], # "Birds fly in sky"
        ]
        
        for i in range(num_samples):
            # Create text by repeating and varying patterns
            text = []
            while len(text) < seq_length:
                pattern = patterns[i % len(patterns)]
                # Add some randomness
                varied_pattern = [t + np.random.randint(0, 50) for t in pattern]
                text.extend(varied_pattern)
            
            text = text[:seq_length]
            self.texts.append(torch.tensor(text, dtype=torch.long))
            
            # Simple thought tensor based on text (32, 32, 16 to match model)
            thought = torch.randn(32, 32, 16, dtype=torch.float32) * 0.1
            # Add structure based on text
            thought += torch.mean(torch.tensor(text, dtype=torch.float32)) * 0.01
            self.thoughts.append(thought)
        
        print(f"Fast dataset created with {len(self.texts)} samples")
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return self.texts[idx], self.thoughts[idx]


def train_simple_real():
    """Simple training function"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Create minimal model
    model = DiffusionThoughtModel()
    model = model.to(device)
    param_count = count_parameters(model)
    print(f"Model: {param_count/1e6:.1f}M parameters")
    
    # Fast dataset
    train_dataset = FastRealDataset(num_samples=2000, seq_length=64, vocab_size=1000)
    eval_dataset = FastRealDataset(num_samples=200, seq_length=64, vocab_size=1000)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)
    
    # Simple noise scheduler
    noise_config = {
        'num_steps': 100,
        'schedule_type': 'linear',
        'beta_start': 0.0001,
        'beta_end': 0.02
    }
    noise_scheduler = create_noise_scheduler(noise_config)
    
    # Move scheduler to device
    if hasattr(noise_scheduler, 'sqrt_alphas_cumprod'):
        noise_scheduler.sqrt_alphas_cumprod = noise_scheduler.sqrt_alphas_cumprod.to(device)
        noise_scheduler.sqrt_one_minus_alphas_cumprod = noise_scheduler.sqrt_one_minus_alphas_cumprod.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    print("\nðŸš€ Starting simple real data training...")
    
    # Training loop
    for epoch in range(3):
        print(f"\n--- Epoch {epoch + 1}/3 ---")
        
        # Training
        model.train()
        train_losses = []
        
        for batch_idx, (tokens, thoughts) in enumerate(tqdm(train_loader, desc="Training")):
            tokens = tokens.to(device)
            thoughts = thoughts.to(device)
            
            # Add noise
            batch_size = tokens.shape[0]
            timesteps = torch.randint(0, noise_scheduler.num_steps, (batch_size,), device=device)
            
            noise = torch.randn_like(thoughts)
            noisy_thoughts = noise_scheduler.add_noise(thoughts, noise, timesteps)
            
            # Forward pass
            pred_logits, pred_thought = model(tokens, noisy_thoughts, timesteps)
            
            # Simple loss (focus on thought reconstruction with dimension reduction)
            # Model outputs (32, 32, 8) but input is (32, 32, 16), so slice the target
            target_thought = thoughts[:, :, :, :8]  # Take first 8 channels
            loss = F.mse_loss(pred_thought, target_thought)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
            
            if batch_idx % 50 == 0:
                print(f"Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        avg_train_loss = np.mean(train_losses)
        
        # Evaluation
        model.eval()
        eval_losses = []
        
        with torch.no_grad():
            for tokens, thoughts in tqdm(eval_loader, desc="Evaluating"):
                tokens = tokens.to(device)
                thoughts = thoughts.to(device)
                
                batch_size = tokens.shape[0]
                timesteps = torch.randint(0, noise_scheduler.num_steps, (batch_size,), device=device)
                
                noise = torch.randn_like(thoughts)
                noisy_thoughts = noise_scheduler.add_noise(thoughts, noise, timesteps)
                
                pred_logits, pred_thought = model(tokens, noisy_thoughts, timesteps)
                target_thought = thoughts[:, :, :, :8]  # Take first 8 channels
                loss = F.mse_loss(pred_thought, target_thought)
                eval_losses.append(loss.item())
        
        avg_eval_loss = np.mean(eval_losses)
        
        print(f"Epoch {epoch + 1} Results:")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        print(f"  Eval Loss: {avg_eval_loss:.4f}")
        print(f"  Memory: {torch.cuda.memory_allocated() / 1024**3:.1f}GB")
    
    # Save results
    results = {
        'final_train_loss': avg_train_loss,
        'final_eval_loss': avg_eval_loss,
        'param_count': param_count,
        'timestamp': datetime.now().isoformat()
    }
    
    os.makedirs('outputs/simple_real_training', exist_ok=True)
    
    # Save model
    model_path = 'outputs/simple_real_training/simple_real_model.pt'
    torch.save(model.state_dict(), model_path)
    
    # Save results
    results_path = 'outputs/simple_real_training/results.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… Simple real data training complete!")
    print(f"Final train loss: {avg_train_loss:.4f}")
    print(f"Final eval loss: {avg_eval_loss:.4f}")
    print(f"Model saved: {model_path}")
    print(f"Results saved: {results_path}")
    
    return results


if __name__ == "__main__":
    results = train_simple_real()