# Enhanced DREAM Configuration for Complex Datasets
# Optimized for datasets requiring long-term memory and complex reasoning

model:
  name: "StackedDiffusionModel3D"
  embed_dim: 768              # Standard transformer size
  num_layers: 12              # Sufficient depth for complex patterns
  num_heads: 12               # Multi-head attention
  vocab_size: 50257           # GPT-2 vocabulary
  max_seq_length: 2048        # Longer sequences to test memory
  max_new_tokens: 1024        # Extended generation capability
  dropout: 0.1                # Standard dropout

# Enhanced thought system for complex reasoning
thought_tensor:
  input_dims: [24, 24, 512]   # Larger thought space for complex patterns
  stack_size: 32              # Deep thought stack for long-term memory
  hidden_dim: 768             # Match embed_dim
  dropout: 0.1                # Lower dropout for complex tasks
  noise_injection_std: 0.01   # Minimal noise for stability
  diversity_loss_weight: 0.05 # Light diversity encouragement
  self_supervised_learning: true
  temporal_dynamics: true     # Critical for memory tasks
  importance_based_retention: true

# DREAM masking strategy for complex data
masking:
  mask_ratio: 0.6             # Moderate initial masking
  confidence_threshold: 0.8   # Higher threshold for complex data
  progressive_unmasking: true
  block_size: 8               # Larger blocks for context
  adaptive_masking: true
  final_mask_ratio: 0.3       # Still challenging final ratio
  final_confidence_threshold: 0.9

# Diffusion optimized for complex reasoning
diffusion:
  num_steps: 1000             # More steps for complex patterns
  noise_schedule: "cosine"
  schedule_type: "cosine"
  beta_start: 0.00085
  beta_end: 0.012
  prediction_type: "epsilon"
  adaptive_timesteps: true

# Training for complex datasets
training:
  batch_size: 2               # Larger batches for stability
  gradient_accumulation_steps: 8  # Effective batch = 16
  learning_rate: 1e-5         # Very conservative LR for stability
  warmup_steps: 50            # Even shorter warmup
  num_epochs: 5
  eval_every: 1
  save_every: 1
  max_grad_norm: 0.25         # Very aggressive gradient clipping
  stack_reset_frequency: 0    # No reset - preserve memory
  label_smoothing: 0.05       # Light smoothing
  early_stopping_patience: 3

# Optimizer
optimizer:
  type: "AdamW"
  weight_decay: 0.01          # Regularization for complex data
  betas: [0.9, 0.999]         # Standard Adam betas
  eps: 1e-8

# Hardware
hardware:
  device: "cuda"
  mixed_precision: true
  gradient_checkpointing: true
  num_workers: 0
  dataloader_pin_memory: false
  compile_model: false
  target_memory_gb: 20

# Memory management
memory:
  offload_to_cpu: false
  cpu_offload_params: false
  activation_checkpointing: true
  thought_stack_checkpointing: true

# Data configuration for complex datasets
data_config:
  cache_dir: "data/cache"
  data_source: "complex"
  train_samples: 10000        # Sufficient data for complex patterns
  eval_samples: 2000

# Advanced features
advanced:
  use_flash_attention: false
  compile_model: false
  deepspeed_config: null
  dream_features: true
  enhanced_thought_system: true
  temporal_attention: true

# Enhanced logging
logging:
  log_every: 50
  use_wandb: false
  use_tensorboard: true
  use_dream_metrics: true
  use_advanced_thought_metrics: true
  wandb_project: "complex-dream-experiment"
  wandb_tags: ["complex", "memory", "reasoning"]
  save_samples: true
  save_thought_visualizations: true
  save_attention_maps: true
  log_train_every: 100
  thought_histogram_every: 500

# Checkpointing
checkpointing:
  checkpoint_dir: "outputs/complex_checkpoints"
  log_dir: "outputs/complex_logs"
  keep_best: true
  keep_latest: 3
  save_optimizer_state: true

# Loss configuration
loss:
  main_loss_weight: 1.0
  thought_loss_weight: 0.0    # Self-supervised only
  confidence_loss_weight: 0.0
  masking_loss_weight: 0.01   # Light masking loss
  temporal_loss_weight: 0.02  # Encourage temporal consistency