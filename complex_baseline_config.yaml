# Baseline Configuration for Complex Datasets (No Thoughts)
# Identical to complex_dataset_config.yaml except removes thought system

model:
  name: "BaselineDreamModel"
  embed_dim: 768              # Standard transformer size
  num_layers: 12              # Sufficient depth for complex patterns
  num_heads: 12               # Multi-head attention
  vocab_size: 50257           # GPT-2 vocabulary
  max_seq_length: 2048        # Longer sequences to test memory
  max_new_tokens: 1024        # Extended generation capability
  dropout: 0.1                # Standard dropout

# DREAM masking strategy for complex data (identical to thought model)
masking:
  mask_ratio: 0.6             # Moderate initial masking
  confidence_threshold: 0.8   # Higher threshold for complex data
  progressive_unmasking: true
  block_size: 8               # Larger blocks for context
  adaptive_masking: true
  final_mask_ratio: 0.3       # Still challenging final ratio
  final_confidence_threshold: 0.9

# Diffusion optimized for complex reasoning (identical)
diffusion:
  num_steps: 1000             # More steps for complex patterns
  noise_schedule: "cosine"
  schedule_type: "cosine"
  beta_start: 0.00085
  beta_end: 0.012
  prediction_type: "epsilon"
  adaptive_timesteps: true

# Training for complex datasets (identical)
training:
  batch_size: 2               # Larger batches for stability
  gradient_accumulation_steps: 8  # Effective batch = 16
  learning_rate: 1e-5         # Very conservative LR for stability
  warmup_steps: 50            # Even shorter warmup
  num_epochs: 5
  eval_every: 1
  save_every: 1
  max_grad_norm: 0.25         # Very aggressive gradient clipping
  stack_reset_frequency: 0    # N/A for baseline
  label_smoothing: 0.05       # Light smoothing
  early_stopping_patience: 3

# Optimizer (identical)
optimizer:
  type: "AdamW"
  weight_decay: 0.01          # Regularization for complex data
  betas: [0.9, 0.999]         # Standard Adam betas
  eps: 1e-8

# Hardware (identical)
hardware:
  device: "cuda"
  mixed_precision: true
  gradient_checkpointing: true
  num_workers: 0
  dataloader_pin_memory: false
  compile_model: false
  target_memory_gb: 20

# Memory management (identical)
memory:
  offload_to_cpu: false
  cpu_offload_params: false
  activation_checkpointing: true
  thought_stack_checkpointing: false  # N/A for baseline

# Data configuration for complex datasets (identical)
data_config:
  cache_dir: "data/cache"
  data_source: "complex"
  train_samples: 10000        # Sufficient data for complex patterns
  eval_samples: 2000

# Advanced features (no thoughts)
advanced:
  use_flash_attention: false
  compile_model: false
  deepspeed_config: null
  dream_features: true
  enhanced_thought_system: false    # No thoughts for baseline
  temporal_attention: false         # No temporal attention without thoughts

# Enhanced logging (identical)
logging:
  log_every: 50
  use_wandb: false
  use_tensorboard: true
  use_dream_metrics: true
  use_advanced_thought_metrics: false  # No thought metrics for baseline
  wandb_project: "complex-baseline-experiment"
  wandb_tags: ["complex", "baseline", "no-thoughts"]
  save_samples: true
  save_thought_visualizations: false   # No thoughts to visualize
  save_attention_maps: true
  log_train_every: 100
  thought_histogram_every: 0           # No thoughts

# Checkpointing (identical)
checkpointing:
  checkpoint_dir: "outputs/complex_baseline_checkpoints"
  log_dir: "outputs/complex_baseline_logs"
  keep_best: true
  keep_latest: 3
  save_optimizer_state: true

# Loss configuration (no thought losses)
loss:
  main_loss_weight: 1.0
  thought_loss_weight: 0.0    # N/A for baseline
  confidence_loss_weight: 0.0
  masking_loss_weight: 0.01   # Light masking loss
  temporal_loss_weight: 0.0   # No temporal loss without thoughts